#include <doctest/doctest.h>
#include "pcg/parallel_computation_graph/parallel_computation_graph_builder.h"
#include "substitutions/apply_substitution/apply_substitution.h"
#include "substitutions/operator_pattern/operator_attribute_constraint.h"
#include "substitutions/output_graph/output_operator_attrs_assignment.h"
#include "substitutions/sub_parallel_computation_graph.h"
#include "substitutions/substitution_builder.h"
#include "substitutions/tensor_pattern/tensor_attribute_pattern.h"
#include "utils/containers/get_only.h"
#include "utils/integer_conversions.h"

using namespace ::FlexFlow;

TEST_SUITE(FF_TEST_SUITE) {
  TEST_CASE("apply_substitution") {
    SubstitutionBuilder b;

    auto [p_input, o_input] = b.add_input(tensor_attribute_pattern_match_all(), "input");
    auto [p_weight, o_weight] = b.add_input(tensor_attribute_pattern_match_all(), "weight");

    PatternValue p_mm_output = [&] {
      auto pattern = OperatorAttributePattern{{
        op_type_equals_constraint(OperatorType::LINEAR),
        op_attr_key_equals(
          OperatorAttributeKey::ACTIVATION, 
          OperatorAttributeValue{std::optional<Activation>{std::nullopt}}
        ),
      }};

      return get_only(
        b.add_pattern_node(pattern, {p_input, p_weight}, {tensor_attribute_pattern_match_all()}, "mm")
      );
    }();

    PatternValue p_relu_output = [&] {
      auto pattern = OperatorAttributePattern{{
        op_type_equals_constraint(OperatorType::RELU),
      }};

      return get_only(
        b.add_pattern_node(pattern, {p_mm_output}, {tensor_attribute_pattern_match_all()}, "relu")
      );
    }();

    OutputGraphExprValue o_fused_output = [&] {
      auto node_expr = OutputOperatorAttrsAssignment{
        b.pattern_node_named("mm"),
        {
          set_attr_to_constant(OperatorAttributeKey::ACTIVATION,
                               OperatorAttributeValue{Activation::RELU}),
        }
      };

      return get_only(b.add_output_graph_node(node_expr, {o_input, o_weight}, 1));
    }();

    b.equate_outputs(p_relu_output, o_fused_output);

    Substitution sub = b.get_substitution();

    int in_channels = 24;
    int batch_size = 4;
    int batch_degree = 2;
    std::string mm_match = "mm_match";
    std::string relu_match = "relu_match";

    SubParallelComputationGraph pcg = [&] {
      ParallelComputationGraphBuilder b;
      parallel_tensor_guid_t t = b.create_input_tensor(ParallelTensorShape{
          ParallelTensorDims{
              FFOrdered<ShardParallelDim>{
                  ShardParallelDim{size_t_from_int(batch_size), batch_degree},
                  ShardParallelDim{size_t_from_int(in_channels), 1},
              },
              ReplicaParallelDimSet{
                  SumDegree{1},
                  DiscardCopyDegree{1},
              },
          },
          DataType::FLOAT,
      });
      t = b.dense(t,
                  /*outDim=*/16,
                  /*activation=*/std::nullopt);
      t = b.gelu(t);
      t = b.dense(t,
                  /*outDim=*/12,
                  /*activation=*/std::nullopt,
                  /*use_bias=*/false,
                  /*data_type=*/DataType::FLOAT,
                  /*kernel_initializer=*/std::nullopt,
                  /*bias_initializer=*/std::nullopt,
                  /*name=*/mm_match);
      t = b.relu(t,
                 /*name=*/relu_match);
      t = b.dense(t,
                  /*outDim=*/8,
                  /*activation=*/Activation::RELU);

      return sub_pcg_from_full_pcg(b.pcg);
    }();

    PCGPatternMatch match = [&] {
      parallel_layer_guid_t mm_match_layer =
          get_parallel_layer_by_name(pcg, mm_match);
      parallel_layer_guid_t relu_match_layer =
          get_parallel_layer_by_name(pcg, relu_match);
      open_parallel_tensor_guid_t mm_match_layer_input_activations =
          get_layer_inputs(pcg, mm_match_layer).at(0);
      open_parallel_tensor_guid_t mm_match_layer_input_weights =
          get_layer_inputs(pcg, mm_match_layer).at(1);

      return PCGPatternMatch{
          bidict<PatternNode, parallel_layer_guid_t>{
              {b.pattern_node_named("mm"), mm_match_layer},
              {b.pattern_node_named("relu"), relu_match_layer},
          },
          std::unordered_map<PatternInput, open_parallel_tensor_guid_t>{
              {
                b.pattern_input_named("input"),
                mm_match_layer_input_activations,
              },
              {
                b.pattern_input_named("weight"),
                mm_match_layer_input_weights,
              }},
      };
    }();

    SubParallelComputationGraph result = apply_substitution(pcg, sub, match);

    SubParallelComputationGraph correct = [&] {
      ParallelComputationGraphBuilder b;
      parallel_tensor_guid_t t = b.create_input_tensor(ParallelTensorShape{
          ParallelTensorDims{
              FFOrdered<ShardParallelDim>{
                  ShardParallelDim{size_t_from_int(batch_size), batch_degree},
                  ShardParallelDim{size_t_from_int(in_channels), 1},
              },
              ReplicaParallelDimSet{
                  SumDegree{1},
                  DiscardCopyDegree{1},
              },
          },
          DataType::FLOAT,
      });
      t = b.dense(t,
                  /*outDim=*/16,
                  /*activation=*/std::nullopt);
      t = b.gelu(t);
      t = b.dense(t,
                  /*outDim=*/12,
                  /*activation=*/Activation::RELU,
                  /*use_bias=*/false,
                  /*data_type=*/DataType::FLOAT,
                  /*kernel_initializer=*/std::nullopt,
                  /*bias_initializer=*/std::nullopt,
                  /*name=*/std::nullopt);
      t = b.dense(t,
                  /*outDim=*/8,
                  /*activation=*/Activation::RELU);

      return sub_pcg_from_full_pcg(b.pcg);
    }();

    // since the new nodes produced by the substitution have new ids, it's
    // easier/more correct to check that the graphs are isomorphic rather than
    // checking their exact graph data
    CHECK(are_isomorphic(result, correct));
  }
}
